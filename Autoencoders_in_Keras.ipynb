{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Autoencoders in Keras"
      ],
      "metadata": {
        "id": "Bh_vBmFVdKQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autoencoder\n",
        "\n",
        "As you read in the introduction, an autoencoder is an unsupervised machine learning algorithm that takes an image as input and tries to reconstruct it using fewer number of bits from the bottleneck also known as latent space. The image is majorly compressed at the bottleneck. The compression in autoencoders is achieved by training the network for a period of time and as it learns it tries to best represent the input image at the bottleneck. The general image compression algorithms like JPEG and JPEG lossless compression techniques compress the images without the need for any kind of training and do fairly well in compressing the images.\n",
        "\n",
        "Autoencoders are similar to dimensionality reduction techniques like Principal Component Analysis (PCA). They project the data from a higher dimension to a lower dimension using linear transformation and try to preserve the important features of the data while removing the non-essential parts.\n",
        "\n",
        "However, the major difference between autoencoders and PCA lies in the transformation part: as you already read, PCA uses linear transformation whereas autoencoders use non-linear transformations.\n",
        "\n",
        "Now that you have a bit of understanding about autoencoders, let's now break this term and try to get some intuition about it!\n",
        "\n",
        "autoencoder keras\n",
        "\n",
        "The above figure is a two-layer vanilla autoencoder with one hidden layer. In deep learning terminology, you will often notice that the input layer is never taken into account while counting the total number of layers in an architecture. The total layers in an architecture only comprises of the number of hidden layers and the ouput layer.\n",
        "\n",
        "As shown in the image above, the input and output layers have the same number of neurons.\n",
        "\n",
        "Let's take an example. You feed an image with just five pixel values into the autoencoder which is compressed by the encoder into three pixel values at the bottleneck (middle layer) or latent space. Using these three values, the decoder tries to reconstruct the five pixel values or rather the input image which you fed as an input to the network.\n",
        "\n",
        "In reality, there are more number of hidden layers in between the input and the output.\n",
        "\n",
        "autoencoder python keras\n",
        "\n",
        "Autoencoder can be broken in to three parts\n",
        "\n",
        "Encoder: this part of the network compresses or downsamples the input into a fewer number of bits. The space represented by these fewer number of bits is often called the latent-space or bottleneck. The bottleneck is also called the \"maximum point of compression\" since at this point the input is compressed the maximum. These compressed bits that represent the original input are together called an “encoding” of the input.\n",
        "Decoder: this part of the network tries to reconstruct the input using only the encoding of the input. When the decoder is able to reconstruct the input exactly as it was fed to the encoder, you can say that the encoder is able to produce the best encodings for the input with which the decoder is able to reconstruct well!\n",
        "There are variety of autoencoders, such as the convolutional autoencoder, denoising autoencoder, variational autoencoder and sparse autoencoder. However, as you read in the introduction, you'll only focus on the convolutional and denoising ones in this tutorial.Convolutional Autoencoders in Python with Keras\n",
        "\n",
        "Since your input data consists of images, it is a good idea to use a convolutional autoencoder. It is not an autoencoder variant, but rather a traditional autoencoder stacked with convolution layers: you basically replace fully connected layers by convolutional layers. Convolution layers along with max-pooling layers, convert the input from wide (a 28 x 28 image) and thin (a single channel or gray scale) to small (7 x 7 image at the latent space) and thick (128 channels).\n",
        "\n",
        "Do not worry if you did not understand the above idea properly! The second part of this tutorial, in which you'll focus on the implementation of the above, will hopefully clear all your doubts (if you have any!).\n",
        "\n",
        "Tip: if you want to know more about convolutional neural networks, check out this tutorial.\n",
        "\n",
        "This helps the network to extract visual features from the images and therefore obtain a more accurate latent space representation. The reconstruction process uses upsampling and convolutions which are known as a decoder. The downsampling is the process in which the image compresses into a low dimension also known as an encoder.\n",
        "\n",
        "It is important to note that the encoder mainly compresses the input image, for example: if your input image is of dimension 176 x 176 x 1 (~30976), then the maximum compression point can have a dimension of 22 x 22 x 512 (~247808). So, in this case, you started with a gray scale image of dimension 176 x 176 and by passing it through a couple of convolutional layers and precisely three max-pooling layers, your image is finally downsampled to a dimension of 22 x 22 but the number of channels are increased from 1 to 512. As stated above, your input from wide (176 x 176) and thin (1) became small (22x22) and thick (512).\n",
        "\n",
        "Loading the Data\n",
        "\n",
        "The notMNIST dataset is an image recognition dataset of font glypyhs for the letters A through J. It is quite similar to the classic MNIST dataset, which contains images of handwritten digits 0 through 9: in this case, you'll find that the NotMNIST dataset comprises 28x28 grayscale images of 70,000 letters from A - J in total 10 categories, and 6,000 images per category.\n",
        "\n",
        "Tip: if you want to learn how to implement an Multi-Layer Perceptron (MLP) for classification tasks with the MNIST dataset, check out this tutorial.\n",
        "\n",
        "The NotMNIST dataset is not predefined in the Keras or the TensorFlow framework, so you'll have to download the data from this source. The data will be downloaded in ubyte.gzip format, but no worries about that just yet! You'll soon learn how to read bytestream formats and convert them into a NumPy array. So, let's get started!\n",
        "\n",
        "The network will be trained on a Nvidia Tesla K40, so if you train on a GPU and use Jupyter Notebook, you will need to add three more lines of code where you specify CUDA device order and CUDA visible devices using a module called os.\n",
        "\n",
        "In the code below, you basically set environment variables in the notebook using os.environ. It's good to do the following before initializing Keras to limit Keras backend TensorFlow to use first GPU. If the machine on which you train on has a GPU on 0, make sure to use 0 instead of 1. You can check that by running a simple command on your terminal: for example, nvidia-smi"
      ],
      "metadata": {
        "id": "Ix5f9CItdK9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" #model will be trained on GPU 1\n"
      ],
      "metadata": {
        "id": "CW83oc1TdPke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import gzip\n",
        "%matplotlib inline\n",
        "from keras.layers import Input,Conv2D,MaxPooling2D,UpSampling2D\n",
        "from keras.models import Model\n",
        "from keras.optimizers import RMSprop\n"
      ],
      "metadata": {
        "id": "zNkfc8N1dRLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data(filename, num_images):\n",
        "    with gzip.open(filename) as bytestream:\n",
        "        bytestream.read(16)\n",
        "        buf = bytestream.read(28 * 28 * num_images)\n",
        "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
        "        data = data.reshape(num_images, 28,28)\n",
        "        return data\n"
      ],
      "metadata": {
        "id": "kBxO5csddSeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = extract_data('train-images-idx3-ubyte.gz', 60000)\n",
        "test_data = extract_data('t10k-images-idx3-ubyte.gz', 10000)\n"
      ],
      "metadata": {
        "id": "U-mA2zqidUq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_labels(filename, num_images):\n",
        "    with gzip.open(filename) as bytestream:\n",
        "        bytestream.read(8)\n",
        "        buf = bytestream.read(1 * num_images)\n",
        "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
        "        return labels\n"
      ],
      "metadata": {
        "id": "B5Tu2QXTdWWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = extract_labels('train-labels-idx1-ubyte.gz',60000)\n",
        "test_labels = extract_labels('t10k-labels-idx1-ubyte.gz',10000)\n"
      ],
      "metadata": {
        "id": "yL_Igk8ldYE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shapes of training set\n",
        "print(\"Training set (images) shape: {shape}\".format(shape=train_data.shape))\n",
        "\n",
        "# Shapes of test set\n",
        "print(\"Test set (images) shape: {shape}\".format(shape=test_data.shape))\n"
      ],
      "metadata": {
        "id": "tpDP_HlidZ5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dictionary of target classes\n",
        "label_dict = {\n",
        " 0: 'A',\n",
        " 1: 'B',\n",
        " 2: 'C',\n",
        " 3: 'D',\n",
        " 4: 'E',\n",
        " 5: 'F',\n",
        " 6: 'G',\n",
        " 7: 'H',\n",
        " 8: 'I',\n",
        " 9: 'J',\n",
        "}\n"
      ],
      "metadata": {
        "id": "O4iyyurMdbhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=[5,5])\n",
        "\n",
        "# Display the first image in training data\n",
        "plt.subplot(121)\n",
        "curr_img = np.reshape(train_data[0], (28,28))\n",
        "curr_lbl = train_labels[0]\n",
        "plt.imshow(curr_img, cmap='gray')\n",
        "plt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")\n",
        "\n",
        "# Display the first image in testing data\n",
        "plt.subplot(122)\n",
        "curr_img = np.reshape(test_data[0], (28,28))\n",
        "curr_lbl = test_labels[0]\n",
        "plt.imshow(curr_img, cmap='gray')\n",
        "plt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")\n"
      ],
      "metadata": {
        "id": "95OwFB99dduN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.reshape(-1, 28,28, 1)\n",
        "test_data = test_data.reshape(-1, 28,28, 1)\n",
        "train_data.shape, test_data.shape\n"
      ],
      "metadata": {
        "id": "QhdertlUdfmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.dtype, test_data.dtype\n"
      ],
      "metadata": {
        "id": "nmvWZqbmdhuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(train_data), np.max(test_data)\n"
      ],
      "metadata": {
        "id": "JJ9fUGjMdjVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data / np.max(train_data)\n",
        "test_data = test_data / np.max(test_data)\n"
      ],
      "metadata": {
        "id": "ad_Mj2dqdl7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(train_data), np.max(test_data)\n"
      ],
      "metadata": {
        "id": "qAmTn_n1dnet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_X,valid_X,train_ground,valid_ground = train_test_split(train_data,\n",
        "                                                             train_data,\n",
        "                                                             test_size=0.2,\n",
        "                                                             random_state=13)\n"
      ],
      "metadata": {
        "id": "m3Q5QAoado1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Autoencoder"
      ],
      "metadata": {
        "id": "KwGjjDFHduSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "epochs = 50\n",
        "inChannel = 1\n",
        "x, y = 28, 28\n",
        "input_img = Input(shape = (x, y, inChannel))\n"
      ],
      "metadata": {
        "id": "83fA2ElwdyLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def autoencoder(input_img):\n",
        "    #encoder\n",
        "    #input = 28 x 28 x 1 (wide and thin)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) #14 x 14 x 32\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small and thick)\n",
        "\n",
        "    #decoder\n",
        "    conv4 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 128\n",
        "    up1 = UpSampling2D((2,2))(conv4) # 14 x 14 x 128\n",
        "    conv5 = Conv2D(64, (3, 3), activation='relu', padding='same')(up1) # 14 x 14 x 64\n",
        "    up2 = UpSampling2D((2,2))(conv5) # 28 x 28 x 64\n",
        "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2) # 28 x 28 x 1\n",
        "    return decoded\n"
      ],
      "metadata": {
        "id": "-KBoMFttdzrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder = Model(input_img, autoencoder(input_img))\n",
        "autoencoder.compile(loss='mean_squared_error', optimizer = RMSprop())\n"
      ],
      "metadata": {
        "id": "SCy7HPgZd1r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.summary()\n"
      ],
      "metadata": {
        "id": "FkHRIxXrd3W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder_train = autoencoder.fit(train_X, train_ground, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_ground))\n"
      ],
      "metadata": {
        "id": "hi1OLOBtd5Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = autoencoder.predict(test_data)\n"
      ],
      "metadata": {
        "id": "8dXBr1kAd_1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred.shape\n"
      ],
      "metadata": {
        "id": "zWiQP_dmeCYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 4))\n",
        "print(\"Test Images\")\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 10, i+1)\n",
        "    plt.imshow(test_data[i, ..., 0], cmap='gray')\n",
        "    curr_lbl = test_labels[i]\n",
        "    plt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")\n",
        "plt.show()\n",
        "plt.figure(figsize=(20, 4))\n",
        "print(\"Reconstruction of Test Images\")\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 10, i+1)\n",
        "    plt.imshow(pred[i, ..., 0], cmap='gray')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ncTXSioQeDs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Denoising Autoencoder"
      ],
      "metadata": {
        "id": "gKYM1IM3eH3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_factor = 0.5\n",
        "x_train_noisy = train_X + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=train_X.shape)\n",
        "x_valid_noisy = valid_X + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=valid_X.shape)\n",
        "x_test_noisy = test_data + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=test_data.shape)\n",
        "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
        "x_valid_noisy = np.clip(x_valid_noisy, 0., 1.)\n",
        "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n"
      ],
      "metadata": {
        "id": "I1XG9P8qeLOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=[5,5])\n",
        "\n",
        "# Display the first image in training data\n",
        "plt.subplot(121)\n",
        "curr_img = np.reshape(x_train_noisy[1], (28,28))\n",
        "plt.imshow(curr_img, cmap='gray')\n",
        "\n",
        "# Display the first image in testing data\n",
        "plt.subplot(122)\n",
        "curr_img = np.reshape(x_test_noisy[1], (28,28))\n",
        "plt.imshow(curr_img, cmap='gray')\n"
      ],
      "metadata": {
        "id": "qRNdwykFeMw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Denoising Autoencoder Network\n",
        "\n",
        "As discussed before the autoencoder is divided into two parts: the encoder and the decoder. The architecture that you're going to construct will look like the following:\n",
        "\n",
        "Encoder\n",
        "\n",
        "The first layer will have 32-3 x 3 filters followed by a downsampling (max-pooling) layer,\n",
        "The second layer will have 64-3 x 3 filters followed by another downsampling layer,\n",
        "The final layer of encoder will have 128-3 x 3 filters.\n",
        "Decoder\n",
        "\n",
        "The first layer will have 128-3 x 3 filters followed by a upsampling layer,\n",
        "The second layer will have 64-3 x 3 filters followed by another upsampling layer,\n",
        "The final layer of encoder will have 1-3 x 3 filters."
      ],
      "metadata": {
        "id": "AAETPNgueRXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "epochs = 20\n",
        "inChannel = 1\n",
        "x, y = 28, 28\n",
        "input_img = Input(shape = (x, y, inChannel))\n"
      ],
      "metadata": {
        "id": "bQgM5p_IeUv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def autoencoder(input_img):\n",
        "    #encoder\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "\n",
        "    #decoder\n",
        "    conv4 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
        "    up1 = UpSampling2D((2,2))(conv4)\n",
        "    conv5 = Conv2D(64, (3, 3), activation='relu', padding='same')(up1)\n",
        "    up2 = UpSampling2D((2,2))(conv5)\n",
        "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2)\n",
        "    return decoded\n"
      ],
      "metadata": {
        "id": "fr0PFxqleWSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder = Model(input_img, autoencoder(input_img))\n",
        "autoencoder.compile(loss='mean_squared_error', optimizer = RMSprop())\n"
      ],
      "metadata": {
        "id": "YmYCufkaeXzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder_train = autoencoder.fit(x_train_noisy, train_X, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_valid_noisy, valid_X))\n"
      ],
      "metadata": {
        "id": "fxAhEnnEeZ5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = autoencoder_train.history['loss']\n",
        "val_loss = autoencoder_train.history['val_loss']\n",
        "epochs = range(epochs)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "92lN9vzdeb9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = autoencoder.predict(x_test_noisy)\n"
      ],
      "metadata": {
        "id": "ydDytAuVee-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 4))\n",
        "print(\"Test Images\")\n",
        "for i in range(10,20,1):\n",
        "    plt.subplot(2, 10, i+1)\n",
        "    plt.imshow(test_data[i, ..., 0], cmap='gray')\n",
        "    curr_lbl = test_labels[i]\n",
        "    plt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")\n",
        "plt.show()\n",
        "plt.figure(figsize=(20, 4))\n",
        "print(\"Test Images with Noise\")\n",
        "for i in range(10,20,1):\n",
        "    plt.subplot(2, 10, i+1)\n",
        "    plt.imshow(x_test_noisy[i, ..., 0], cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(20, 4))\n",
        "print(\"Reconstruction of Noisy Test Images\")\n",
        "for i in range(10,20,1):\n",
        "    plt.subplot(2, 10, i+1)\n",
        "    plt.imshow(pred[i, ..., 0], cmap='gray')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Qmb4wfzhegw2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}